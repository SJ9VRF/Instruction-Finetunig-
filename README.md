# Instruction Finetuning




![Screenshot_2024-08-08_at_4 13 39_PM-removebg-preview](https://github.com/user-attachments/assets/d46b9725-5936-4edc-a4e5-81c41693f520)


# Fine-Tuning Llama 3

This project demonstrates how to fine-tune the Meta-Llama-3-8B-Instruct model on a legal summarization task. The goal is to adapt this large language model to efficiently summarize legal documents, improving accessibility and comprehension of legal texts.

## Project Overview

The objective of this repository is to showcase the fine-tuning process using a specific dataset that includes pairs of detailed legal texts and their respective concise summaries. This could be particularly useful for legal professionals or anyone interested in automated text summarization in specialized domains.

## Getting Started

### Prerequisites

- Python 3.7 or higher
- Jupyter Notebook or JupyterLab
- GPU acceleration (recommended for efficient training)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/your-username/FineTuning_Llama3_Legal_Summarization.git

2. **Navigate to the project directory:**
   ```bash
   cd FineTuning_Llama3_Legal_Summarization
3. **Install the required packages:**
   ```bash
   cd FineTuning_Llama3_Legal_Summarization

## Running the Notebook
Open the `FineTuning_Llama3_Legal_Summarization.ipynb` notebook in Jupyter and execute the cells to perform fine-tuning and evaluation.

## Dataset
The project utilizes the "Legal Summarization" dataset, originally sourced from academic publications on legal document summarization. It includes pairs of lengthy legal texts and their summaries, structured to train models for summarization tasks effectively.

## Features
- **Model Adaptation**: Techniques to adapt Meta-Llama-3-8B-Instruct for summarizing legal texts.
- **Performance Evaluation**: Metrics and methods to evaluate the model's summarization quality on legal documents.
- **Fine-Tuning Guidance**: Step-by-step instructions to fine-tune and test large language models on niche datasets.
